---
layout: publication
title: "Subspace Snooping: Exploiting Temporal Sharing Stability for Snoop Reduction"
image: 
authors:
  - name: Jeongseob Ahn
    short: J. Ahn
  - name: Daehoon Kim
    short: D. Kim
  - name: Jaehong Kim
    short: J. Kim
  - name: Jaehyuk Huh
    short: J. Huh
type: Paper
international: true
paper:
  year: 2012
  publisher: IEEE Transaction on Computers
  publisher-type: Journal
  publisher-short: TC
  ref: vol. 61, no. 11, pp. 1624-1637
researches: [memory]
keywords:
  - Snoop-based coherence protocol
  - Subspace snooping
  - Cache-to-cache transfers
  - Memory page table
  - Coherence protocol
  - Token coherence
  - Shrinking mechanisms
  - Speculative shrinking
sidebar:
  doi: 10.1109/TC.2011.195
---

Although snoop-based coherence protocols provide fast cache-to-cache transfers with a simple and robust coherence mechanism, scaling the protocols has been difficult due to the overheads of broadcast snooping. In this paper, we propose a coherence filtering technique called subspace snooping, which stores the potential sharers of each memory page in the page table entry. By using the sharer information in the page table entry, coherence transactions for a page generate snoop requests only to the subset of nodes in the system. However, the coherence subspace of a page may evolve, as the phases of applications may change or the operating system may migrate threads to different nodes. To adjust subspaces dynamically, subspace snooping supports two different shrinking mechanisms, which remove obsolete nodes from subspaces. Among the two shrinking mechanisms, subspace snooping with safe shrinking can be integrated to any type of coherence protocols and network topologies, as it guarantees that a subspace always contains the precise sharers of a page. Speculative shrinking breaks the subspace superset property, but achieves better snoop reductions than safe shrinking. We evaluate subspace snooping with Token Coherence on unordered mesh networks. Subspace snooping reduces 58 percent of snoops on average for a set of parallel scientific and server workloads, and 87 percent for our multiprogrammed workloads.