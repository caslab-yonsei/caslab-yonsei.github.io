---
layout: publication
title: "RoPIM: A Processing-in-Memory Architecture for Accelerating Rotary Positional Embedding in Transformer Models"
image:
authors:
  - name: YunHyeong Jeon
  - name: Minwoo Jang
  - name: Hwanjun Lee
  - name: Yeji Jung
  - name: Jin Jung
  - name: Jonggeon Lee
  - name: Jinin So
  - name: Daehoon Kim
    corresponding: true
    
co-first: false
type: Paper
international: true
researches: [memory, accelerators]
keywords:
  - Processing-in-memory
  - AI HW accelerators
  - Rotary Positional Embedding
  - Transformer Models
paper:
  year: 2025
  publisher: "IEEE Computer Architecture Letters"
  publisher-short: "CAL"
  publisher-type: Journal
doi: 10.1109/LCA.2025.3535470
sidebar:
components: [abstract, keywords, topics]
hidden: false
---

The emergence of attention-based Transformer models, such as GPT, BERT, and LLaMA, has revolutionized Natural Language Processing (NLP) by significantly improving performance across a wide range of applications. A critical factor driving these improvements is the use of positional embeddings, which are crucial for capturing the contextual relationships between tokens in a sequence. However, current positional embedding methods face challenges, particularly in managing performance overhead for long sequences and effectively capturing relationships between adjacent tokens. In response, Rotary Positional Embedding (RoPE) has emerged as a method that effectively embeds positional information with high accuracy and without necessitating model retraining even with long sequences. Despite its effectiveness, RoPE introduces a considerable performance bottleneck during inference. We observe that RoPE accounts for 61% of GPU execution time due to extensive data movement and execution dependencies. In this paper, we introduce RoPIM, a Processing-In-Memory (PIM) architecture designed to efficiently accelerate RoPE operations in Transformer models. RoPIM achieves this by utilizing a bank-level accelerator that reduces off-chip data movement through in-accelerator support for multiply-addition operations and minimizes operational dependencies via parallel data rearrangement. Additionally, RoPIM proposes an optimized data mapping strategy that leverages both bank-level and row-level mappings to enable parallel execution, eliminate bank-to-bank communication, and reduce DRAM activations. Our experimental results show that RoPIM achieves up to a 307.9× performance improvement and 914.1× energy savings compared to conventional systems.
